{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.classification import accuracy_score, log_loss\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv('training_variants')\n",
    "data_text =pd.read_csv(\"training_text\",sep=\"\\|\\|\",engine=\"python\",names=[\"ID\",\"TEXT\"],skiprows=1)\n",
    "\n",
    "# loading stop words from nltk library\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def nlp_preprocessing(total_text, index, column):\n",
    "    if type(total_text) is not int:\n",
    "        string = \"\"\n",
    "        # replace every special char with space\n",
    "        total_text = re.sub('[^a-zA-Z0-9\\n]', ' ', str(total_text))\n",
    "        # replace multiple spaces with single space\n",
    "        total_text = re.sub('\\s+',' ', total_text)\n",
    "        # converting all the chars into lower-case.\n",
    "        total_text = total_text.lower()\n",
    "        \n",
    "        for word in total_text.split():\n",
    "        # if the word is a not a stop word then retain that word from the data\n",
    "            if not word in stop_words:\n",
    "                string += word + \" \"\n",
    "        \n",
    "        data_text[column][index] = string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#text processing stage.\n",
    "start_time = time.clock()\n",
    "for index, row in data_text.iterrows():#iterate over row in dataframe pandas\n",
    "    nlp_preprocessing(row['TEXT'], index, 'TEXT')\n",
    "\n",
    "#merging both gene_variations and text data based on ID\n",
    "result = pd.merge(data, data_text,on='ID', how='left')\n",
    "\n",
    "y_true = result['Class'].values\n",
    "result.Gene      = result.Gene.str.replace('\\s+', '_')\n",
    "result.Variation = result.Variation.str.replace('\\s+', '_')\n",
    "\n",
    "# split the data into test and train by maintaining same distribution of output varaible 'y_true' [stratify=y_true]\n",
    "X_train, test_df, y_train, y_test = train_test_split(result, y_true, stratify=y_true, test_size=0.2)\n",
    "# split the train data into train and cross validation by maintaining same distribution of output varaible 'y_train' [stratify=y_train]\n",
    "train_df, cv_df, y_train, y_cv = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# one-hot encoding of Gene feature.\n",
    "gene_vectorizer = CountVectorizer()\n",
    "train_gene_feature_onehotCoding = gene_vectorizer.fit_transform(train_df['Gene'])\n",
    "test_gene_feature_onehotCoding = gene_vectorizer.transform(test_df['Gene'])\n",
    "cv_gene_feature_onehotCoding = gene_vectorizer.transform(cv_df['Gene'])\n",
    "\n",
    "\n",
    "# one-hot encoding of variation feature.\n",
    "variation_vectorizer = CountVectorizer()\n",
    "train_variation_feature_onehotCoding = variation_vectorizer.fit_transform(train_df['Variation'])\n",
    "test_variation_feature_onehotCoding = variation_vectorizer.transform(test_df['Variation'])\n",
    "cv_variation_feature_onehotCoding = variation_vectorizer.transform(cv_df['Variation'])\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "def extract_dictionary_paddle(cls_text):\n",
    "    dictionary = defaultdict(int)\n",
    "    for index, row in cls_text.iterrows():\n",
    "        for word in row['TEXT'].split():\n",
    "            dictionary[word] +=1\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# building a CountVectorizer with all the words that occured minimum 3 times in train data\n",
    "text_vectorizer = CountVectorizer(min_df=3)\n",
    "train_text_feature_onehotCoding = text_vectorizer.fit_transform(train_df['TEXT'])\n",
    "# getting all the feature names (words)\n",
    "train_text_features= text_vectorizer.get_feature_names()\n",
    "\n",
    "# train_text_feature_onehotCoding.sum(axis=0).A1 will sum every row and returns (1*number of features) vector\n",
    "train_text_fea_counts = train_text_feature_onehotCoding.sum(axis=0).A1\n",
    "\n",
    "# zip(list(text_features),text_fea_counts) will zip a word with its number of times it occured\n",
    "text_fea_dict = dict(zip(list(train_text_features),train_text_fea_counts))\n",
    "\n",
    "\n",
    "\n",
    "dict_list = []\n",
    "# dict_list =[] contains 9 dictoinaries each corresponds to a class\n",
    "for i in range(1,10):\n",
    "    cls_text = train_df[train_df['Class']==i]\n",
    "    # build a word dict based on the words in that class\n",
    "    dict_list.append(extract_dictionary_paddle(cls_text))\n",
    "    # append it to dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dict_list[i] is build on i'th  class text data\n",
    "# total_dict is buid on whole training text data\n",
    "total_dict = extract_dictionary_paddle(train_df)\n",
    "\n",
    "\n",
    "confuse_array = []\n",
    "for i in train_text_features:\n",
    "    ratios = []\n",
    "    max_val = -1\n",
    "    for j in range(0,9):\n",
    "        ratios.append((dict_list[j][i]+10 )/(total_dict[i]+90))\n",
    "    confuse_array.append(ratios)\n",
    "confuse_array = np.array(confuse_array)\n",
    "\n",
    "# don't forget to normalize every feature\n",
    "train_text_feature_onehotCoding = normalize(train_text_feature_onehotCoding, axis=0)\n",
    "\n",
    "# we use the same vectorizer that was trained on train data\n",
    "test_text_feature_onehotCoding = text_vectorizer.transform(test_df['TEXT'])\n",
    "# don't forget to normalize every feature\n",
    "test_text_feature_onehotCoding = normalize(test_text_feature_onehotCoding, axis=0)\n",
    "\n",
    "# we use the same vectorizer that was trained on train data\n",
    "cv_text_feature_onehotCoding = text_vectorizer.transform(cv_df['TEXT'])\n",
    "# don't forget to normalize every feature\n",
    "cv_text_feature_onehotCoding = normalize(cv_text_feature_onehotCoding, axis=0)\n",
    "\n",
    "sorted_text_fea_dict = dict(sorted(text_fea_dict.items(), key=lambda x: x[1] , reverse=True))\n",
    "sorted_text_occur = np.array(list(sorted_text_fea_dict.values()))\n",
    "\n",
    "\n",
    "train_gene_var_onehotCoding = hstack((train_gene_feature_onehotCoding,train_variation_feature_onehotCoding))\n",
    "test_gene_var_onehotCoding = hstack((test_gene_feature_onehotCoding,test_variation_feature_onehotCoding))\n",
    "cv_gene_var_onehotCoding = hstack((cv_gene_feature_onehotCoding,cv_variation_feature_onehotCoding))\n",
    "\n",
    "train_x_onehotCoding = hstack((train_gene_var_onehotCoding, train_text_feature_onehotCoding)).tocsr()\n",
    "train_y = np.array(list(train_df['Class']))\n",
    "\n",
    "test_x_onehotCoding = hstack((test_gene_var_onehotCoding, test_text_feature_onehotCoding)).tocsr()\n",
    "test_y = np.array(list(test_df['Class']))\n",
    "\n",
    "cv_x_onehotCoding = hstack((cv_gene_var_onehotCoding, cv_text_feature_onehotCoding)).tocsr()\n",
    "cv_y = np.array(list(cv_df['Class']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for alpha = 1e-05\n",
      "Log Loss : 1.302486664191609\n",
      "for alpha = 0.0001\n",
      "Log Loss : 1.29769961445265\n",
      "for alpha = 0.001\n",
      "Log Loss : 1.2949829691796635\n",
      "for alpha = 0.1\n",
      "Log Loss : 1.269458471337792\n",
      "for alpha = 1\n",
      "Log Loss : 1.2970400277907674\n",
      "for alpha = 10\n",
      "Log Loss : 1.372703945845657\n",
      "for alpha = 100\n",
      "Log Loss : 1.3673831660690778\n",
      "for alpha = 1000\n",
      "Log Loss : 1.3069679345363816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True),\n",
       "            cv=3, method='sigmoid')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "alpha = [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000]\n",
    "cv_log_error_array = []\n",
    "for i in alpha:\n",
    "    print(\"for alpha =\", i)\n",
    "    clf = MultinomialNB(alpha=i)\n",
    "    clf.fit(train_x_onehotCoding, train_y)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(train_x_onehotCoding, train_y)\n",
    "    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
    "    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n",
    "    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n",
    "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n",
    "\n",
    "best_alpha = np.argmin(cv_log_error_array)\n",
    "clf = MultinomialNB(alpha=alpha[best_alpha])\n",
    "clf.fit(train_x_onehotCoding, train_y)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(train_x_onehotCoding, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Predicted Class : 1\n",
      "Predicted Class Probabilities: [[0.3725 0.1299 0.02   0.1764 0.056  0.054  0.1805 0.0061 0.0046]]\n",
      "0.3725\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
    "# to avoid rounding error while multiplying probabilites we use log-probability estimates\n",
    "#print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\n",
    "#print(\"Number of missclassified point :\", np.count_nonzero((sig_clf.predict(cv_x_onehotCoding)- cv_y))/cv_y.shape[0])\n",
    "#plot_confusion_matrix(cv_y, sig_clf.predict(cv_x_onehotCoding.toarray()))\n",
    "\n",
    "g=pd.Series('FAM58A')\n",
    "v=pd.Series('Truncating Mutations')\n",
    "st=pd.Series('hi')\n",
    "'''\n",
    "g=pd.Series('TERT')\n",
    "v=pd.Series('C228T')\n",
    "st=pd.Series(result.iloc[28][4])'''\n",
    "\n",
    "test_gene_feature_onehotCoding = gene_vectorizer.transform(g)\n",
    "test_variation_feature_onehotCoding = variation_vectorizer.transform(v)\n",
    "test_text_feature_onehotCoding = text_vectorizer.transform(st)\n",
    "test_text_feature_onehotCoding = normalize(test_text_feature_onehotCoding, axis=0)\n",
    "test_gene_var_onehotCoding = hstack((test_gene_feature_onehotCoding,test_variation_feature_onehotCoding))\n",
    "test_x_onehotCoding = hstack((test_gene_var_onehotCoding, test_text_feature_onehotCoding)).tocsr()\n",
    "\n",
    "no_feature = 100\n",
    "predicted_cls = sig_clf.predict(test_x_onehotCoding[0])\n",
    "if predicted_cls[0]==7:\n",
    "    print(predicted_cls,'1')\n",
    "else:\n",
    "    print(0)\n",
    "print(\"Predicted Class :\", predicted_cls[0])\n",
    "l=np.round(sig_clf.predict_proba(test_x_onehotCoding[0]),4)\n",
    "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[0]),4))\n",
    "print(l.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "naive_pickle=open('naive_pickle_file.pkl','wb')\n",
    "pickle.dump(clf,naive_pickle)\n",
    "naive_pickle.close()\n",
    "G_Vectorizer_pickle=open('gene_vec_pickle_file.pkl','wb')\n",
    "pickle.dump(gene_vectorizer,G_Vectorizer_pickle)\n",
    "G_Vectorizer_pickle.close()\n",
    "V_Vectorizer_pickle=open('var_vec_pickle_file.pkl','wb')\n",
    "pickle.dump(variation_vectorizer,V_Vectorizer_pickle)\n",
    "V_Vectorizer_pickle.close()\n",
    "T_Vectorizer_pickle=open('report_vec_pickle_file.pkl','wb')\n",
    "pickle.dump(text_vectorizer,T_Vectorizer_pickle)\n",
    "T_Vectorizer_pickle.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sig_clf_pickle=open('sig_clf_pickle_file.pkl','wb')\n",
    "pickle.dump(sig_clf,sig_clf_pickle)\n",
    "T_Vectorizer_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import hstack\n",
    "import pickle\n",
    "\n",
    "g=pd.Series('BRCA1')\n",
    "v=pd.Series('p53')\n",
    "st=pd.Series('hii')\n",
    "\n",
    "test_gene_feature_onehotCoding = gene_vectorizer.transform(g)\n",
    "test_variation_feature_onehotCoding = variation_vectorizer.transform(v)\n",
    "test_text_feature_onehotCoding = text_vectorizer.transform(st)\n",
    "test_text_feature_onehotCoding = normalize(test_text_feature_onehotCoding, axis=0)\n",
    "test_gene_var_onehotCoding = hstack((test_gene_feature_onehotCoding,test_variation_feature_onehotCoding))\n",
    "test_x_onehotCoding = hstack((test_gene_var_onehotCoding, test_text_feature_onehotCoding)).tocsr()\n",
    "\n",
    "no_feature = 100\n",
    "predicted_cls = sig_clf.predict(test_x_onehotCoding[0])\n",
    "print(\"Predicted Class :\", predicted_cls[0])\n",
    "l=np.round(sig_clf.predict_proba(test_x_onehotCoding[0]),4)\n",
    "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[0]),4))\n",
    "print(l.max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
